import torch
import numpy as np

from torch import nn
from transformers.modeling_outputs import Seq2SeqLMOutput
from transformers import get_linear_schedule_with_warmup, get_constant_schedule

from models.mttod import ModelBase, T5WithTokenSpan
from data_utils.mttod.utils import definitions
from utils.utils import get_or_create_logger


logger = get_or_create_logger(__name__)


class T5WithResponsePG(T5WithTokenSpan):
	def __init__(self, config, num_span, special_token_ids):
		super(T5WithResponsePG, self).__init__(config, num_span)
		self._special_token_ids = special_token_ids

	def get_tod_errors(self, resp_pred, resp_label):
		resp_pred = resp_pred.detach().cpu().numpy()
		resp_label = resp_label.detach().cpu().numpy()

		special_token_mask = np.isin(resp_label, self._special_token_ids)
		# error per row = turn generated by model
		num_critical_errors = np.sum(special_token_mask * (resp_pred != resp_label), axis=1, keepdims=True)
		# divide by max so it is between 0 and 1
		num_critical_errors = num_critical_errors / np.max(num_critical_errors)
		return torch.from_numpy(num_critical_errors)

	def reward_weighted_action_loss(self, lm_logits, lm_labels):
		# computing the greedt next word probabilities
		greedy_probs = torch.softmax(lm_logits, dim=-1) # [batch_size, prob_of_each_word]
		greedy_next_word_probs = greedy_probs[torch.arange(greedy_probs.shape[0]), torch.argmax(greedy_probs, dim=1)]
		non_weighted_loss = -1.0 * torch.log(greedy_next_word_probs) # because log(p) is negative

		errors_per_turn = self.get_tod_errors(torch.argmax(lm_logits, dim=-1), lm_labels)
		errors_per_turn = errors_per_turn.to(lm_logits.device)

		# for logging
		total_normed_errors_per_batch = torch.sum(errors_per_turn) / errors_per_turn.shape[0]

		return torch.sum(non_weighted_loss * errors_per_turn) / lm_logits.shape[0], total_normed_errors_per_batch

	def forward(self,
				input_ids=None,
				attention_mask=None,
				decoder_input_ids=None,
				encoder_outputs=None,
				past_key_values=None,
				inputs_embeds=None,
				decoder_inputs_embeds=None,
				span_labels=None,
				lm_labels=None,
				use_cache=None,
				output_attentions=None,
				output_hidden_states=None,
				return_dict=None,
				encoder_only=None,
				add_auxiliary_task=None,
				decoder_type=None):

		use_cache = use_cache if use_cache is not None else self.config.use_cache
		return_dict = return_dict if return_dict is not None else self.config.return_dict

		span_loss, pred_spans, span_logits = 0, None, None

		if encoder_outputs is None:
			encoder_outputs = self.encoder(input_ids=input_ids,
										   attention_mask=attention_mask,
										   inputs_embeds=inputs_embeds,
										   return_dict=return_dict)

			if return_dict:
				encoder_hidden_states = encoder_outputs.last_hidden_state
			else:
				encoder_hidden_states = encoder_outputs[0]

			# encoder forward to obtain last hidden state for each token
			hs = encoder_hidden_states * (self.model_dim ** -0.5)

			if add_auxiliary_task:
				# loss for span prediction, which is encoder_hidden_state + linear head
				span_loss, pred_spans, span_logits = self.predict_span(
					hs, attention_mask, span_labels)

		else:
			if isinstance(encoder_outputs, tuple):
				encoder_hidden_states = encoder_outputs[0]
			else:
				encoder_hidden_states = encoder_outputs.last_hidden_state

		if encoder_only:
			return (span_loss, pred_spans, span_logits), encoder_outputs

		if lm_labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
			decoder_input_ids = self._shift_right(lm_labels) # want input starts from <sos>, hence shift right

		if decoder_type == "resp":
			decoder = self.resp_decoder # an additional decoder for response
			lm_head = self.resp_lm_head
		else:
			decoder = self.decoder # the original decoder and lm head for T5
			lm_head = self.lm_head

		if past_key_values is not None:
			assert lm_labels is None, "Decoder should not use cached key value states when training"
			if decoder_input_ids is not None:
				decoder_input_ids = decoder_input_ids[:, -1:]
			if decoder_inputs_embeds is not None:
				decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]

		if decoder_type == "resp" and False: # TODO: full decoding
			with torch.no_grad():
				# greedy decode for response
				decoder_input_ids = self._prepare_decoder_input_ids_for_generation(
					lm_labels.shape[0], # batch size
					device=lm_labels.device
				)
				decoder_attention_mask = attention_mask
				stopping_criteria = self._get_stopping_criteria(
					max_length=lm_labels.shape[1], max_time=None, stopping_criteria=[]
				)
				logits_processor = self._get_logits_processor(
					repetition_penalty=None,
					no_repeat_ngram_size=None,
					encoder_no_repeat_ngram_size=None,
					input_ids_seq_length=1,
					encoder_input_ids=None,
					bad_words_ids=None,
					min_length=0,
					max_length=lm_labels.shape[1],
					eos_token_id=self.config.eos_token_id,
					forced_bos_token_id=None,
					forced_eos_token_id=None,
					prefix_allowed_tokens_fn=None,
					num_beams=1,
					num_beam_groups=1,
					diversity_penalty=None,
					remove_invalid_values=None,
					exponential_decay_length_penalty=None,
					logits_processor=[],
					renormalize_logits=None,
				)
				greedy_resp_ids = self.greedy_search(
					input_ids = decoder_input_ids,
					logits_processor = logits_processor,
					stopping_criteria = stopping_criteria,
					pad_token_id = self.config.pad_token_id,
					eos_token_id = self.config.eos_token_id,
					attention_mask = decoder_attention_mask,
					encoder_outputs = encoder_outputs,
				)
				# print('greedy_resp_ids', greedy_resp_ids) # is correct

		decoder_outputs = decoder(input_ids=decoder_input_ids,
								  inputs_embeds=decoder_inputs_embeds,
								  past_key_values=past_key_values,
								  encoder_hidden_states=encoder_hidden_states,
								  encoder_attention_mask=attention_mask,
								  use_cache=use_cache,
								  return_dict=return_dict)

		sequence_output = decoder_outputs[0]

		sequence_output = sequence_output * (self.model_dim ** -0.5)

		lm_logits = lm_head(sequence_output) # final prediction = probability of each token

		lm_loss = 0.0
		total_normed_errors_per_batch = 0.0
		if lm_labels is not None:
			if decoder_type == "resp": # policy gradient
				# probability of the response tokens
				lm_logits_resp = lm_logits.view(-1, lm_logits.size(-1))
				policy_loss, total_normed_errors_per_batch = self.reward_weighted_action_loss(lm_logits_resp, lm_labels)
				# the above only makes model confused what is correct

				lm_loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)
				lm_loss = lm_loss_fct(
					lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
				lm_loss = lm_loss + policy_loss
			else: # normal language modelling
				lm_loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)
				lm_loss = lm_loss_fct(
					lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))

		# for training
		if not return_dict:
			pred_lm = torch.argmax(lm_logits, dim=-1)
			outputs = (lm_loss, pred_lm, total_normed_errors_per_batch) + \
				(span_loss, pred_spans, span_logits, encoder_hidden_states) + \
					decoder_outputs[1:]

		# for prediction
		else:
			outputs = Seq2SeqLMOutput(
				loss=lm_loss,
				logits=lm_logits,
				past_key_values=decoder_outputs.past_key_values,
				decoder_hidden_states=decoder_outputs.hidden_states,
				decoder_attentions=decoder_outputs.attentions,
				cross_attentions=decoder_outputs.cross_attentions,
				encoder_last_hidden_state=encoder_outputs.last_hidden_state,
				encoder_hidden_states=encoder_outputs[1] if len(
					encoder_outputs) > 1 else None,
				encoder_attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)

		return outputs


class PGMTTODModel(ModelBase):
	def __init__(self, config, reader_vocab_size, tokenizer):
		super(PGMTTODModel, self).__init__(config)

		if self.cfg.ckpt is not None:
			model_path = self.cfg.ckpt
			initialize_additional_decoder = False
		elif self.cfg.train_from is not None:
			model_path = self.cfg.train_from
			initialize_additional_decoder = False
		else:
			model_path = self.cfg.backbone
			initialize_additional_decoder = True

		num_span = len(definitions.EXTRACTIVE_SLOT)
		special_token_ids = [v for k, v in tokenizer.get_added_vocab().items() if k.startswith('[') and not k.startswith('[db') and not k.startswith('[PAD]')]
		self.model = T5WithResponsePG.from_pretrained(model_path, num_span=num_span, special_token_ids=special_token_ids)
		# align token ids, needed since GODEL uses a different pad_token_id than t5
		self.model.config.pad_token_id = tokenizer.pad_token_id
		self.model.config.eos_token_id = tokenizer.eos_token_id
		self.model.config.decoder_start_token_id = tokenizer.pad_token_id

		self.model.resize_token_embeddings(reader_vocab_size)
		if initialize_additional_decoder:
			self.model.initialize_additional_decoder()
		
		self.model.to(self.cfg.device)
		return

	def __call__(self, *args, **kwargs):
		return self.model(*args, **kwargs)

	def forward(self, *args, **kwargs):
		return self.model(*args, **kwargs)

	def generate(self, *args, **kwargs):
		return self.model.generate(*args, **kwargs)

	def save_pretrained(self, *args, **kwargs):
		return self.model.save_pretrained(*args, **kwargs)

	def get_optimizer_and_scheduler(self, num_traininig_steps_per_epoch):
		num_train_steps = (num_traininig_steps_per_epoch *
			self.cfg.epochs) // self.cfg.grad_accum_steps

		if self.cfg.warmup_steps >= 0:
			num_warmup_steps = self.cfg.warmup_steps
		else:
			num_warmup_steps = int(num_train_steps * self.cfg.warmup_ratio)

		logger.info(f"Total training steps = {num_train_steps}, {num_warmup_steps=}")

		optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.cfg.learning_rate)

		if self.cfg.no_learning_rate_decay:
			scheduler = get_constant_schedule(optimizer)
		else:
			scheduler = get_linear_schedule_with_warmup(
				optimizer,
				num_warmup_steps=num_warmup_steps,
				num_training_steps=num_train_steps)

		return optimizer, scheduler

	