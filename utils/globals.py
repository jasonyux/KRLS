class _MTTOD_CONFIG:
	backbone = "t5-base"
	version = "2.1"
	data_dir = "dataset"
	task = "e2e"
	repetition_penalty = 1.0
	use_tod_loss_scale = True
	add_auxiliary_task = True
	context_size = -1
	ururu = False
	batch_size = 4
	epochs = 10
	warmup_steps = -1
	warmup_ratio = 0.2
	learning_rate = 0.0005
	weight_decay = 0.0
	grad_accum_steps = 1
	max_grad_norm = 1.0
	aux_loss_coeff = 0.5
	resp_loss_coeff = 1.0
	num_train_dialogs = -1
	train_from = None
	no_validation = False
	no_learning_rate_decay = False
	pred_data_type = "test"
	overwrite_with_span = False
	beam_size = 1
	do_sample = False
	top_k = 0
	top_p = 0.7
	temperature = 1.0
	use_true_dbpn = False
	use_true_curr_aspn = False
	use_true_prev_bspn = False
	use_true_prev_aspn = False
	use_true_prev_resp = False
	top_n = 5
	output = None
	run_type = "train"
	excluded_domains = None
	save_model = True
	model_dir = "model_checkpoints/mttod_cp/my"
	seed = 42
	deterministic = False
	ckpt = None
	log_file = None
	log_frequency = 100
	max_to_keep_ckpt = 2
	num_gpus = 1
	device = "cuda:0"
	sep_act_n_resp_gen = False
	resp_use_bspn = False
	val_watch_key = "total_score"
	score_each_dialog = False
	debug = False
	pilot_run = False


class _PPOTOD_CONFIG:
	backbone = "t5-base"
	version = "2.1"
	data_dir = "dataset"
	exp_group_name = "ppotod"
	task = "e2e"
	repetition_penalty = 1.0
	use_tod_loss_scale = None # should not be used
	add_auxiliary_task = True
	context_size = -1
	ururu = False
	batch_size = 4
	epochs = 10
	warmup_steps = -1
	warmup_ratio = 0.2
	learning_rate = 0.0005
	weight_decay = 0.0
	grad_accum_steps = 1
	max_grad_norm = 1.0
	aux_loss_coeff = 0.5
	resp_loss_coeff = 10.0
	add_kl_divergence = False
	precompute_kl_logits = False  # switched to using TinyDB which saves memory
	precompute_db_path = "model_checkpoints/ppotod/precompute/baseline"
	kl_ckpt = "model_checkpoints/ppotod/baseline/ckpt-epoch8"
	kl_loss_coeff = 0.1
	use_lm = True   # should we use LM Loss in our learning loop?
	use_ppo = True  # if use_ppo is False and use lm is True, then it is basically a baseline with only LM
	is_policy_optimization = None  # if None then it is set to (self.cfg.use_ppo and not self.cfg.use_lm), else it is set to this value
	freeze_encoder = False
	use_gold_prob = 0.0  # if use_gold_prob=0.0, then experience collection is just greedy decoding with ground truth
	add_gold_demonstrations = False  # if True, then we add gold demonstrations to the experience pool
	update_old_policy_interval = 1  # no wait
	lm_head_model_action_value = False # whether lm head should model action value
	action_value_lm_head_init = True
	lm_head_init = True  # initialize the LM head with backbone weights
	alternate_epoch = True  # alterante RL and LM per epoch or per step
	alternate_step_k = 0.5  # if > 1, then alternate RL and LM per k steps. If float, then it is a ratio
	ppo_epoch = 1
	reward = "token_prob"  # sentence_error for each token or token_error for each token
	correct_reward = 1.0
	token_prob_temperature = 0.8  # used to reshape the token probability distribution for the ref model
	token_prob_scale = 0.8  # used to reshape the token probability distribution for the ref model
	precompute_token_prob_logits = False 
	token_embedding_path = "model_checkpoints/ppotod/embeddings/baseline_embedding_layer.pt"
	lm_scale = "none"  # just normal CE loss for LM. if sentence_error, scale the CE loss by sentence_error. Used mostly for baseline cmp
	penalize_da_tokens = False  # if true, resp_token_mask also include da tokens
	add_terminal_reward = False  # use BLEU + Sp_token_f1 as terminal reward
	terminal_reward_scale = 5.0
	terminal_reward_fn = "sp_f1"  # bleu + sp_f1 = "all"
	use_bert_score = False
	sample_action = 'greedy'  # sample or greedy while collecting experience
	num_per_sample = 3  # number of samples per action/data point
	sample_top_p = 0.9
	sample_temperature = 0.2
	sample_temperature_decay = 1.0  # multiply by this after each epoch. 1.0 means no decay
	special_token_error_scale = 5.0  # relative importance of special tokens, 1.0 means equal importance
	rl_gamma = 0.99
	rl_lambda = 0.95
	normalize_advantage = True
	normalize_return = True
	real_normalize_return = False
	adv_use_returns = False
	ppo_clip = 0.2
	val_loss_coeff = 0.5
	action_val_loss_coeff = 1.0
	rollout_train_epochs = 1
	num_train_dialogs = -1
	train_from = None
	no_validation = False
	no_predict = False
	no_learning_rate_decay = False
	pred_data_type = "test"
	sep_act_n_resp_gen = False  # if true, generate act and then generate resp
	overwrite_with_span = False
	beam_size = 1
	do_sample = False
	top_k = 0
	top_p = 0.7
	temperature = 1.0
	use_true_dbpn = False
	use_true_curr_aspn = False
	use_true_prev_bspn = False
	use_true_prev_aspn = False
	use_true_prev_resp = False
	top_n = 5
	output = None
	run_type = "train"
	excluded_domains = None
	save_model = True
	model_dir = "model_checkpoints/mttod_cp/my"
	seed = 42
	dropout = 0.1  # model dropout
	deterministic = False
	ckpt = None
	log_file = None
	log_frequency = 100
	max_to_keep_ckpt = 3
	num_gpus = 1
	device = "cuda:0"
	resp_use_bspn = False
	val_watch_key = "total_score"  # "resp.correct"
	save_val_predictions = True
	score_each_dialog = False  # give a score for each dialog, useful when de
	debug = False
	skip_val_predictions = False  # only for debugging
	save_sampled_trajectories = False  # debug
	pilot_run = False


class _PGTOD_CONFIG:
	backbone = "t5-base"
	version = "2.1"
	data_dir = "dataset"
	exp_group_name = "ppotod"
	task = "e2e"
	repetition_penalty = 1.0
	use_tod_loss_scale = None # should not be used
	add_auxiliary_task = True
	context_size = -1
	ururu = False
	batch_size = 4
	epochs = 10
	warmup_steps = -1
	warmup_ratio = 0.2
	learning_rate = 0.0005
	weight_decay = 0.0
	grad_accum_steps = 1
	max_grad_norm = 1.0
	aux_loss_coeff = 0.5
	resp_loss_coeff = 10.0
	add_kl_divergence = False
	precompute_kl_logits = False  # switched to using TinyDB which saves memory
	precompute_db_path = "model_checkpoints/ppotod/precompute/baseline"
	kl_ckpt = "model_checkpoints/ppotod/baseline/ckpt-epoch8"
	kl_loss_coeff = 0.1
	use_lm = True   # should we use LM Loss in our learning loop?
	use_ppo = True  # if use_ppo is False and use lm is True, then it is basically a baseline with only LM
	is_policy_optimization = None  # if None then it is set to (self.cfg.use_ppo and not self.cfg.use_lm), else it is set to this value
	freeze_encoder = False
	use_gold_prob = 0.0  # if use_gold_prob=0.0, then experience collection is just greedy decoding with ground truth
	add_gold_demonstrations = False  # if True, then we add gold demonstrations to the experience pool
	update_old_policy_interval = 1  # no wait
	lm_head_model_action_value = False # whether lm head should model action value
	action_value_lm_head_init = True
	lm_head_init = True  # initialize the LM head with backbone weights
	alternate_epoch = True  # alterante RL and LM per epoch or per step
	alternate_step_k = 0.5  # if > 1, then alternate RL and LM per k steps. If float, then it is a ratio
	ppo_epoch = 1
	reward = "token_prob"  # sentence_error for each token or token_error for each token
	correct_reward = 1.0
	token_prob_temperature = 0.8  # used to reshape the token probability distribution for the ref model
	token_prob_scale = 0.8  # used to reshape the token probability distribution for the ref model
	precompute_token_prob_logits = False 
	token_embedding_path = "model_checkpoints/ppotod/embeddings/baseline_embedding_layer.pt"
	lm_scale = "none"  # just normal CE loss for LM. if sentence_error, scale the CE loss by sentence_error. Used mostly for baseline cmp
	penalize_da_tokens = False  # if true, resp_token_mask also include da tokens
	add_terminal_reward = False  # use BLEU + Sp_token_f1 as terminal reward
	terminal_reward_scale = 5.0
	terminal_reward_fn = "sp_f1"  # bleu + sp_f1
	use_bert_score = False
	sample_action = 'greedy'  # sample or greedy while collecting experience
	num_per_sample = 3  # number of samples per action/data point
	sample_top_p = 0.9
	sample_temperature = 0.2
	sample_temperature_decay = 1.0  # multiply by this after each epoch. 1.0 means no decay
	special_token_error_scale = 5.0  # relative importance of special tokens, 1.0 means equal importance
	rl_algo = "ssemi-off-policy"
	rl_gamma = 0.99
	rl_lambda = 0.95
	normalize_advantage = True
	normalize_return = True
	real_normalize_return = False
	adv_use_returns = False
	ppo_clip = 0.2
	val_loss_coeff = 0.5
	action_val_loss_coeff = 1.0
	rollout_train_epochs = 1
	num_train_dialogs = -1
	train_from = None
	no_validation = False
	no_predict = False
	no_learning_rate_decay = False
	pred_data_type = "test"
	sep_act_n_resp_gen = False  # if true, generate act and then generate resp
	overwrite_with_span = False
	beam_size = 1
	do_sample = False
	top_k = 0
	top_p = 0.7
	temperature = 1.0
	use_true_dbpn = False
	use_true_curr_aspn = False
	use_true_prev_bspn = False
	use_true_prev_aspn = False
	use_true_prev_resp = False
	top_n = 5
	output = None
	run_type = "train"
	excluded_domains = None
	save_model = True
	model_dir = "model_checkpoints/mttod_cp/my"
	seed = 42
	dropout = 0.1  # model dropout
	deterministic = False
	ckpt = None
	log_file = None
	log_frequency = 100
	max_to_keep_ckpt = 3
	num_gpus = 1
	device = "cuda:0"
	resp_use_bspn = False
	val_watch_key = "total_score"  # "resp.correct"
	save_val_predictions = True
	score_each_dialog = False  # give a score for each dialog, useful when de
	debug = False
	skip_val_predictions = False  # only for debugging
	save_sampled_trajectories = False  # debug
	pilot_run = False